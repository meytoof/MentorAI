# üöÄ Configuration Ollama pour l'IA

## Installation d'Ollama

1. **T√©l√©charger Ollama** : https://ollama.ai/download
2. **Installer** selon votre syst√®me d'exploitation
3. **D√©marrer Ollama** (il d√©marre automatiquement en service)

## T√©l√©charger un mod√®le

Pour utiliser l'IA, vous devez t√©l√©charger un mod√®le. Recommandations :

```bash
# Mod√®le l√©ger et rapide (recommand√© pour commencer)
ollama pull llama3.2

# Mod√®le plus puissant (n√©cessite plus de RAM)
ollama pull llama3.1

# Mod√®le sp√©cialis√© pour le fran√ßais
ollama pull mistral
```

## Configuration dans le projet

Ajoutez ces variables dans votre fichier `.env` :

```env
# URL d'Ollama (par d√©faut : http://localhost:11434)
OLLAMA_URL=http://localhost:11434

# Mod√®le √† utiliser (doit correspondre √† un mod√®le t√©l√©charg√©)
OLLAMA_MODEL=llama3.2
```

## V√©rifier que Ollama fonctionne

```bash
# Tester Ollama directement
ollama run llama3.2 "Bonjour, peux-tu m'aider ?"

# Ou via curl
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Bonjour"
}'
```

## D√©pannage

### Ollama ne r√©pond pas

- V√©rifiez que le service Ollama est d√©marr√©
- V√©rifiez le port 11434 : `curl http://localhost:11434/api/tags`

### Le mod√®le n'est pas trouv√©

- Liste les mod√®les install√©s : `ollama list`
- T√©l√©chargez le mod√®le manquant : `ollama pull <nom-du-mod√®le>`

### Erreurs de m√©moire

- Utilisez un mod√®le plus petit (llama3.2 au lieu de llama3.1)
- R√©duisez la taille du contexte dans le code si n√©cessaire

## Mode d√©veloppement sans Ollama

Si Ollama n'est pas disponible, l'API `/api/assist` utilisera automatiquement un fallback avec des indices g√©n√©riques. L'application fonctionnera toujours, mais sans l'intelligence artificielle.

---

## üåê Utiliser Ollama avec un front d√©ploy√© sur Vercel

### Important : Ollama ne tourne pas sur Vercel

- Vercel ne permet pas d‚Äôinstaller et d‚Äôex√©cuter Ollama directement sur leurs fonctions serverless.
- **Conclusion** : Ollama doit tourner **ailleurs** (chez toi ou sur un serveur d√©di√©), et ton app Next.js (sur Vercel) doit juste l‚Äôappeler via HTTP.

### 1. Sc√©nario simple : Vercel (front) + Ollama chez toi (dev perso)

En prod Vercel, `localhost` ne marche plus, car `localhost` pointerait vers les serveurs Vercel, pas ta machine.

1. Fais tourner Ollama chez toi (comme en dev).
2. Expose ton Ollama de mani√®re s√©curis√©e (recommandation : via un tunnel ou un reverse proxy avec auth, pas directement sur Internet).
3. Mets une URL accessible publiquement dans les variables d‚Äôenvironnement Vercel :

Dans Vercel ‚Üí _Project Settings_ ‚Üí _Environment Variables_ :

```env
OLLAMA_URL=https://ton-domaine-ou-tunnel.exemple.com
OLLAMA_MODEL=llama3.2
```

Dans ton `.env.local` (pour dev) tu peux garder :

```env
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

### 2. Sc√©nario plus propre : backend d√©di√© pour Ollama

Architecture recommand√©e :

- **Vercel** : h√©berge uniquement le front Next.js (ton app actuelle).
- **Server d√©di√© / VPS / machine chez toi** : fait tourner Ollama + √©ventuellement une petite API Node/Express, FastAPI, etc.

Le flux devient :

1. Front (Vercel) ‚Üí appelle `/api/assist` (route Next.js).
2. `/api/assist` ‚Üí appelle ton backend Ollama (URL dans `OLLAMA_URL`).
3. Ton backend parle √† Ollama, renvoie la r√©ponse JSON √† `/api/assist`, qui la renvoie au front.

Avantages :

- Tu ma√Ætrises l‚Äôh√©bergement d‚ÄôOllama (RAM, GPU, s√©curit√©).
- Tu peux mettre de l‚Äôauth, du rate limiting, des logs, etc. devant Ollama.

### 3. S√©curiser l‚Äôacc√®s √† Ollama

**√Ä √©viter absolument :**

- Exposer `http://ton-ip:11434` brut sur Internet sans protection.

**√Ä faire :**

- Mettre un reverse proxy (NGINX, Caddy, Traefik‚Ä¶) devant Ollama.
- Ajouter au minimum :
  - Auth par token/API key
  - HTTPS (Let‚Äôs Encrypt)
  - Limite d‚ÄôIP ou VPN si possible

Exemple (id√©e de base c√¥t√© Next.js) :

- Ajouter une variable secr√®te c√¥t√© Vercel :

```env
OLLAMA_API_KEY=un_token_que_tu_verifies_cote_serveur
```

- Et ne transmettre ce token qu‚Äôentre `/api/assist` et ton backend Ollama (jamais au navigateur).

### 4. R√©sum√© local vs Vercel

- **Dev local :**

  - Ollama tourne sur ta machine.
  - `.env.local` :
    - `OLLAMA_URL=http://localhost:11434`
    - `OLLAMA_MODEL=llama3.2`

- **Prod Vercel :**
  - Front sur Vercel.
  - Ollama sur une autre machine/serveur.
  - Variables Vercel :
    - `OLLAMA_URL=https://ton-backend-ollama.exemple.com`
    - `OLLAMA_MODEL=llama3.2`
    - `OLLAMA_API_KEY=...` (optionnel mais recommand√©)

Si tu veux, je peux aussi te proposer une petite API Node (ou autre) pr√™te √† d√©ployer sur un VPS pour servir d‚Äôinterface propre entre Vercel et Ollama.
